{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1bfd3m7l5YVL"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.classify import NaiveBayesClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_documents = [\"The movie was great and highly enjoyable.\", \"I loved the book; it was fantastic.\"]\n",
        "negative_documents = [\"The concert was terrible and disappointing.\", \"The service at the restaurant was awful.\"]\n",
        "\n",
        "# Combine positive and negative documents into one list\n",
        "documents = [(doc, \"Positive\") for doc in positive_documents] + [(doc, \"Negative\") for doc in negative_documents]"
      ],
      "metadata": {
        "id": "vV-TQSgj5cne"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Tokenize and preprocess the documents\n",
        "all_words = []\n",
        "for document, sentiment in documents:\n",
        "    words = word_tokenize(document)\n",
        "    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
        "    all_words.extend(words)\n",
        "\n",
        "# Extract the most common words as features\n",
        "word_features = FreqDist(all_words).most_common(100)\n",
        "word_features = [word for word, _ in word_features]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUuGOIsY5cpp",
        "outputId": "7327aea4-57ab-47f4-990e-f6c1e19f750b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def document_features(document):\n",
        "    document_words = set(word_tokenize(document.lower()))\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['contains({})'.format(word)] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "# Generate feature sets for the documents\n",
        "feature_sets = [(document_features(doc), sentiment) for doc, sentiment in documents]"
      ],
      "metadata": {
        "id": "4VNNlSDL5crF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = NaiveBayesClassifier.train(feature_sets)"
      ],
      "metadata": {
        "id": "1tzqgQJI5ctL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example document to classify\n",
        "new_document = \"The movie was terrible and boring.\"\n",
        "\n",
        "# Preprocess and extract features from the new document\n",
        "new_features = document_features(new_document)\n",
        "\n",
        "# Classify the new document using the trained classifier\n",
        "classification = classifier.classify(new_features)\n",
        "print(\"Document sentiment:\", classification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V4ZCygB5cvZ",
        "outputId": "2f955606-a3c2-448c-c0bc-582363d5b43a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document sentiment: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WAIfLy3K5cxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UWsgJn7f55W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnMeMpnD55ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiCJV34T55c2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}